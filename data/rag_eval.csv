questions
What is 'jailbreaking' in the context of Direct Prompt Injections?
How can unauthorized access to sensitive data occur as a result of LLM outputs?
How might poorly-engineered prompts lead to Excessive Agency in LLM applications?
What are hallucination and confabulation in relation to LLM outputs?
What methods can malicious actors use to compromise LLM models?
